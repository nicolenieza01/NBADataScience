# -*- coding: utf-8 -*-
"""Tmodels.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vma-gAorugMvNJksaGkYTn7nj8vu9hG8
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers

import pandas as pd
import matplotlib.pyplot as plt

from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.model_selection import cross_validate, cross_val_score, train_test_split
from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, Ridge
from sklearn.ensemble import AdaBoostRegressor, AdaBoostClassifier
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier

from gensim.models import Word2Vec


import torch
from transformers import BertTokenizer, BertModel


import numpy as np

AllTextData = pd.read_csv('/content/drive/MyDrive/448 - Project/Tdata/AllTextData.csv')
TextAndNBA = pd.read_csv('/content/drive/MyDrive/448 - Project/Tdata/TextPlusNBA.csv')

print(len(AllTextData[AllTextData['year'] == 2021]))

TextAndNBA[f"AoBPR"] = (TextAndNBA["PR"] > TextAndNBA["PR"].mean()).astype(int)

TextAndNBA["AoBAvPR"].hist()

# show the histogram
plt.show()


TextAndNBA["AoBPTS"].hist()

# show the histogram
plt.show()

TextAndNBA["PTS"].hist()

# show the histogram
plt.show()

Performance = ["FG%", "PR", "3P%", "PF", "PTS"]

PercentTrain = 0.8
np.random.seed(6)
TrainIndex = np.random.choice(len(TextAndNBA),  int(len(TextAndNBA) * PercentTrain), replace=False)

print(len(TextAndNBA))
print(len(TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), 'removed_text']))

sum(TextAndNBA['AoBPR'] / len(TextAndNBA))

"""# TF-IDF Unigram:"""

tfidf_vectorizer = TfidfVectorizer(ngram_range =(1,1))

# fit and transform the text data to calculate the TF-IDF unigram scores
tfidf_unigrams = tfidf_vectorizer.fit_transform(TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), 'removed_text'])
print(tfidf_unigrams.shape)

# EXTRA BELOW:

unigrams = tfidf_vectorizer.get_feature_names_out()


# create a Pandas dataframe to store the TF-IDF unigram scores
tfidf_unigram_df = pd.DataFrame(tfidf_unigrams.toarray(), columns=unigrams)

df2 = TfidfVectorizer(ngram_range =(1,1), min_df=2)

# fit and transform the text data to calculate the TF-IDF unigram scores
tfidf2_unigrams = df2.fit_transform(TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), 'removed_text'])
print(tfidf2_unigrams.shape)

df3 = TfidfVectorizer(ngram_range =(1,1), min_df=3)

# fit and transform the text data to calculate the TF-IDF unigram scores
tfidf3_unigrams = df3.fit_transform(TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), 'removed_text'])
print(tfidf3_unigrams.shape)


df4 = TfidfVectorizer(ngram_range =(1,1), min_df=4)

# fit and transform the text data to calculate the TF-IDF unigram scores
tfidf4_unigrams = df4.fit_transform(TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), 'removed_text'])
print(tfidf4_unigrams.shape)


df5 = TfidfVectorizer(ngram_range =(1,1), min_df=5)

# fit and transform the text data to calculate the TF-IDF unigram scores
tfidf5_unigrams = df5.fit_transform(TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), 'removed_text'])
print(tfidf5_unigrams.shape)

# For test data


# fit and transform the text data to calculate the TF-IDF unigram scores
tfidf_unigramsTest = tfidf_vectorizer.transform(TextAndNBA.loc[~TextAndNBA.index.isin(TrainIndex), 'removed_text'])
print(tfidf_unigramsTest.shape)

tfidf2_unigramsTest = df2.transform(TextAndNBA.loc[~TextAndNBA.index.isin(TrainIndex), 'removed_text'])

tfidf3_unigramsTest = df3.transform(TextAndNBA.loc[~TextAndNBA.index.isin(TrainIndex), 'removed_text'])

tfidf4_unigramsTest = df4.transform(TextAndNBA.loc[~TextAndNBA.index.isin(TrainIndex), 'removed_text'])

tfidf5_unigramsTest = df5.transform(TextAndNBA.loc[~TextAndNBA.index.isin(TrainIndex), 'removed_text'])

"""# TF-IDF Bigram:"""

tfidf_vectorizer_bi = TfidfVectorizer(ngram_range =(2,2))

# fit and transform the text data to calculate the TF-IDF unigram scores
tfidf_bigrams = tfidf_vectorizer_bi.fit_transform(TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), 'removed_text'])
print(tfidf_bigrams.shape)

# EXTRA BELOW:

# get the feature names (i.e., the unigrams)
#x_array =
#print(tfidf_unigrams.toarray())
bigrams = tfidf_vectorizer_bi.get_feature_names_out()


# create a Pandas dataframe to store the TF-IDF unigram scores
tfidf_bigram_df = pd.DataFrame(tfidf_bigrams.toarray(), columns=bigrams)

dfb2 = TfidfVectorizer(ngram_range =(2,2), min_df=2)

# fit and transform the text data to calculate the TF-IDF unigram scores
tfidf2_bigrams = dfb2.fit_transform(TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), 'removed_text'])
print(tfidf2_bigrams.shape)

dfb3 = TfidfVectorizer(ngram_range =(2,2), min_df=3)

# fit and transform the text data to calculate the TF-IDF unigram scores
tfidf3_bigrams = dfb3.fit_transform(TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), 'removed_text'])
print(tfidf3_bigrams.shape)


dfb4 = TfidfVectorizer(ngram_range =(2,2), min_df=4)

# fit and transform the text data to calculate the TF-IDF unigram scores
tfidf4_bigrams = dfb4.fit_transform(TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), 'removed_text'])
print(tfidf4_bigrams.shape)


dfb5 = TfidfVectorizer(ngram_range =(2,2), min_df=5)

# fit and transform the text data to calculate the TF-IDF unigram scores
tfidf5_bigrams = dfb5.fit_transform(TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), 'removed_text'])
print(tfidf5_bigrams.shape)

tfidf_bigramsTest = tfidf_vectorizer_bi.transform(TextAndNBA.loc[~TextAndNBA.index.isin(TrainIndex), 'removed_text'])
print(tfidf_bigramsTest.shape)


tfidf2_bigramsTest = dfb2.transform(TextAndNBA.loc[~TextAndNBA.index.isin(TrainIndex), 'removed_text'])

tfidf3_bigramsTest = dfb3.transform(TextAndNBA.loc[~TextAndNBA.index.isin(TrainIndex), 'removed_text'])

tfidf4_bigramsTest = dfb4.transform(TextAndNBA.loc[~TextAndNBA.index.isin(TrainIndex), 'removed_text'])

tfidf5_bigramsTest = dfb5.transform(TextAndNBA.loc[~TextAndNBA.index.isin(TrainIndex), 'removed_text'])

"""# TD-IDF Uni and Bigrams"""

tfidf_vectorizer_ubi = TfidfVectorizer(ngram_range =(1,2))

# fit and transform the text data to calculate the TF-IDF unigram scores
tfidf_ubigrams = tfidf_vectorizer_ubi.fit_transform(TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), 'removed_text'])
print(tfidf_ubigrams.shape)

# EXTRA BELOW:

# get the feature names (i.e., the unigrams)
#x_array =
#print(tfidf_unigrams.toarray())
ubigrams = tfidf_vectorizer_ubi.get_feature_names_out()


# create a Pandas dataframe to store the TF-IDF unigram scores
tfidf_ubigram_df = pd.DataFrame(tfidf_ubigrams.toarray(), columns=ubigrams)

dfub2 = TfidfVectorizer(ngram_range =(1,2), min_df=2)

# fit and transform the text data to calculate the TF-IDF unigram scores
tfidf2_ubigrams = dfub2.fit_transform(TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), 'removed_text'])
print(tfidf2_ubigrams.shape)

dfub3 = TfidfVectorizer(ngram_range =(1,2), min_df=3)

# fit and transform the text data to calculate the TF-IDF unigram scores
tfidf3_ubigrams = dfub3.fit_transform(TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), 'removed_text'])
print(tfidf3_ubigrams.shape)


dfub4 = TfidfVectorizer(ngram_range =(1,2), min_df=4)

# fit and transform the text data to calculate the TF-IDF unigram scores
tfidf4_ubigrams = dfub4.fit_transform(TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), 'removed_text'])
print(tfidf4_ubigrams.shape)


dfub5 = TfidfVectorizer(ngram_range =(1,2), min_df=5)

# fit and transform the text data to calculate the TF-IDF unigram scores
tfidf5_ubigrams = dfub5.fit_transform(TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), 'removed_text'])
print(tfidf5_ubigrams.shape)

tfidf_ubigramsTest = tfidf_vectorizer_ubi.transform(TextAndNBA.loc[~TextAndNBA.index.isin(TrainIndex), 'removed_text'])
print(tfidf_ubigramsTest.shape)


tfidf2_ubigramsTest = dfub2.transform(TextAndNBA.loc[~TextAndNBA.index.isin(TrainIndex), 'removed_text'])

tfidf3_ubigramsTest = dfub3.transform(TextAndNBA.loc[~TextAndNBA.index.isin(TrainIndex), 'removed_text'])

tfidf4_ubigramsTest = dfub4.transform(TextAndNBA.loc[~TextAndNBA.index.isin(TrainIndex), 'removed_text'])

tfidf5_ubigramsTest = dfub5.transform(TextAndNBA.loc[~TextAndNBA.index.isin(TrainIndex), 'removed_text'])

"""# Word2vec:"""

text_data = TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), 'removed_text'].values.tolist()
tokenized_data = [text.split() for text in text_data]
VECTOR_SIZE = 60
model = Word2Vec(tokenized_data, min_count=3, vector_size=VECTOR_SIZE)
embedded_data = []
for text in tokenized_data:
    embedded_text = [model.wv[word] for word in text if word in model.wv.key_to_index]
    embedded_data.append(embedded_text)

lengts = np.empty(0)
for text in embedded_data:
  lengts = np.append(lengts, len(text))

print(lengts.max())
print(lengts.min())
MAXWORDS = int(lengts.max())

word2vec_embed = np.empty((0,MAXWORDS,VECTOR_SIZE))
for document in embedded_data:
  doc_vec = np.empty((0, VECTOR_SIZE))
  for word in document:

    doc_vec = np.vstack((doc_vec, np.reshape(word, (1, VECTOR_SIZE))))

  if doc_vec.shape[0] < MAXWORDS:
    n = MAXWORDS - doc_vec.shape[0]
    doc_vec = np.pad(doc_vec, [(0, n), (0 , 0)], mode = 'constant')

  elif doc_vec.shape[0] > MAXWORDS:
    doc_vec = doc_vec[:MAXWORDS, :]



  word2vec_embed = np.vstack((word2vec_embed, np.reshape(doc_vec, (1, MAXWORDS, VECTOR_SIZE))))

word2vec_embed = np.reshape(word2vec_embed, (word2vec_embed.shape[0],  word2vec_embed.shape[1] * word2vec_embed.shape[2]   ))
print(word2vec_embed.shape)

text_data = TextAndNBA.loc[~TextAndNBA.index.isin(TrainIndex), 'removed_text'].values.tolist()
tokenized_data = [text.split() for text in text_data]
model = Word2Vec(tokenized_data, min_count=3, vector_size=VECTOR_SIZE)
embedded_data = []
for text in tokenized_data:
    embedded_text = [model.wv[word] for word in text if word in model.wv.key_to_index]
    embedded_data.append(embedded_text)


word2vec_embedTest = np.empty((0,MAXWORDS,VECTOR_SIZE))
for document in embedded_data:
  doc_vec = np.empty((0, VECTOR_SIZE))
  for word in document:

    doc_vec = np.vstack((doc_vec, np.reshape(word, (1, VECTOR_SIZE))))

  if doc_vec.shape[0] < MAXWORDS:
    n = MAXWORDS - doc_vec.shape[0]
    doc_vec = np.pad(doc_vec, [(0, n), (0 , 0)], mode = 'constant')

  elif doc_vec.shape[0] > MAXWORDS:
    doc_vec = doc_vec[:MAXWORDS, :]



  word2vec_embedTest = np.vstack((word2vec_embedTest, np.reshape(doc_vec, (1, MAXWORDS, VECTOR_SIZE))))



word2vec_embedTest = np.reshape(word2vec_embedTest, (word2vec_embedTest.shape[0],  word2vec_embedTest.shape[1] * word2vec_embedTest.shape[2]   ))
print(word2vec_embedTest.shape)

"""# Testing word2vec models"""

X = big_vec
#print(X)
y = np.array(TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), 'AoBPR'])

# Split the data into training and test sets
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
#print(X_train.shape)
# Train the logistic regression model

#print(X_train.shape)

logmodel = LogisticRegression(penalty='l1',max_iter=300, solver='liblinear')
logmodel.fit(X, y)

# Evaluate the model
#y_pred = model.predict(X_test)

#accuracy = (y_pred == y_test).mean()



for perf in Performance:
  # Extract the tf-idf scores and the AoBPR label from your dataframe
  X = big_vec
  y = TextAndNBA[f"AoB{perf}"][TrainIndex].values
  print(f"For perf: {perf}")
  # Split the data into training and testing sets

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

  # Create an instance of the logistic regression model
  logreg = LogisticRegression(penalty='l1',max_iter=300, solver='liblinear')

  # Fit the model to the training data
  #logreg.fit(X_train, y_train)

  # Predict the labels for the test data
  #y_pred = logreg.predict(X_test)

  scores = cross_val_score(logreg, X, y, cv=10)



  print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))

text_data = TextAndNBA.loc[~TextAndNBA.index.isin(TrainIndex), 'removed_text'].values.tolist()
tokenized_data = [text.split() for text in text_data]
VECTOR_SIZE = 50
model = Word2Vec(tokenized_data, min_count=3, vector_size=VECTOR_SIZE)
embedded_data = []
for text in tokenized_data:
    embedded_text = [model.wv[word] for word in text if word in model.wv.key_to_index]
    embedded_data.append(embedded_text)


big_vec = np.empty((0,MAXWORDS,VECTOR_SIZE))
for document in embedded_data:
  doc_vec = np.empty((0, VECTOR_SIZE))
  for word in document:

    doc_vec = np.vstack((doc_vec, np.reshape(word, (1, VECTOR_SIZE))))

  if doc_vec.shape[0] < MAXWORDS:
    n = MAXWORDS - doc_vec.shape[0]
    doc_vec = np.pad(doc_vec, [(0, n), (0 , 0)], mode = 'constant')

  elif doc_vec.shape[0] > MAXWORDS:
    doc_vec = doc_vec[:MAXWORDS, :]



  big_vec = np.vstack((big_vec, np.reshape(doc_vec, (1, MAXWORDS, VECTOR_SIZE))))


print(np.reshape(big_vec, (big_vec.shape[0],  big_vec.shape[1] * big_vec.shape[2]   )).shape)
big_vec = np.reshape(big_vec, (big_vec.shape[0],  big_vec.shape[1] * big_vec.shape[2]   ))

X_test = big_vec

#for perf in Performance:

y_test = TextAndNBA.loc[~TextAndNBA.index.isin(TrainIndex), f'AoBPR']


y_pred = logmodel.predict(X_test)




accuracy = (y_pred == y_test).mean()
print(f"Test {perf} accuracy: {accuracy}")

vec_range = [20, 40, 60, 80, 100, 120]

for VECTOR_SIZE in vec_range:
  print(f"ATTEMPTING VECTOR SIZE {VECTOR_SIZE}")
  model = Word2Vec(tokenized_data, min_count=3, vector_size=VECTOR_SIZE)
  embedded_data = []
  for text in tokenized_data:
      embedded_text = [model.wv[word] for word in text if word in model.wv.key_to_index]
      embedded_data.append(embedded_text)
  lengts = np.empty(0)
  for text in embedded_data:
    lengts = np.append(lengts, len(text))



  big_vec = np.empty((0,MAXWORDS,VECTOR_SIZE))
  for document in embedded_data:
    doc_vec = np.empty((0, VECTOR_SIZE))
    for word in document:
      doc_vec = np.vstack((doc_vec, np.reshape(word, (1, VECTOR_SIZE))))
    if doc_vec.shape[0] < MAXWORDS:
      n = MAXWORDS - doc_vec.shape[0]
      doc_vec = np.pad(doc_vec, [(0, n), (0 , 0)], mode = 'constant')
    big_vec = np.vstack((big_vec, np.reshape(doc_vec, (1, MAXWORDS, VECTOR_SIZE))))
  big_vec = np.reshape(big_vec, (big_vec.shape[0],  big_vec.shape[1] * big_vec.shape[2]   ))



  for perf in Performance:
    # Extract the tf-idf scores and the AoBPR label from your dataframe
    X = big_vec
    y = TextAndNBA[f"AoB{perf}"].values
    print(f"For perf: {perf}")
    # Split the data into training and testing sets

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Create an instance of the logistic regression model
    logreg = LogisticRegression(penalty='l1',max_iter=300, solver='liblinear')

    # Fit the model to the training data
    #logreg.fit(X_train, y_train)

    # Predict the labels for the test data
    #y_pred = logreg.predict(X_test)

    scores = cross_val_score(logreg, X, y, cv=10)



    print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))

"""# BERT Vectorization"""

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

## TAKES AROUND 10 MINUTES


# Load text data from pandas DataFrame
texts = TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), 'removed_text'].tolist()

# Tokenize text using BERT tokenizer
tokens = tokenizer.batch_encode_plus(texts,
                                      add_special_tokens=True,
                                      padding=True,
                                      truncation=True,
                                      return_tensors='pt')

# Generate BERT embeddings for the input text
with torch.no_grad():
    embeddings = model(tokens['input_ids'], attention_mask=tokens['attention_mask'])[0]

# Extract the embeddings for the first token (the [CLS] token)
bert_embeddings = embeddings[:, 0, :].numpy()

print(bert_embeddings.shape)

texts = TextAndNBA.loc[~TextAndNBA.index.isin(TrainIndex), 'removed_text'].tolist()

# Tokenize text using BERT tokenizer
tokens = tokenizer.batch_encode_plus(texts,
                                      add_special_tokens=True,
                                      padding=True,
                                      truncation=True,
                                      return_tensors='pt')

# Generate BERT embeddings for the input text
with torch.no_grad():
    embeddings = model(tokens['input_ids'], attention_mask=tokens['attention_mask'])[0]

# Extract the embeddings for the first token (the [CLS] token)
bert_embeddingsTest = embeddings[:, 0, :].numpy()
print(bert_embeddingsTest.shape)

"""# Testing BERT Data"""

np.save("/content/drive/MyDrive/448 - Project/Tdata/BERTEmbeddings.npy")

X = bert_embeddings
#print(X)
y = np.array(TextAndNBA['AoBPR'])

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
#print(X_train.shape)
# Train the logistic regression model

print(X_train.shape)

model = LogisticRegression(penalty='l1',max_iter=300, solver='liblinear')
model.fit(X_train, y_train)

# Evaluate the model
y_pred = model.predict(X_test)

accuracy = (y_pred == y_test).mean()

for perf in Performance:
  # Extract the tf-idf scores and the AoBPR label from your dataframe
  X = bert_embeddings
  y = TextAndNBA[f"AoB{perf}"].values
  print(f"For perf: {perf}")
  # Split the data into training and testing sets

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

  # Create an instance of the logistic regression model
  logreg = LogisticRegression(penalty='l1',max_iter=300, solver='liblinear')

  # Fit the model to the training data
  #logreg.fit(X_train, y_train)

  # Predict the labels for the test data
  #y_pred = logreg.predict(X_test)

  scores = cross_val_score(logreg, X, y, cv=10)



  print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))

"""# Models

## Digging into tfidf
"""

ALL_DATA = {"unigram_tfidf": tfidf_unigrams, "unigram_tfidf2": tfidf2_unigrams, "unigram_tfidf3": tfidf3_unigrams, "unigram_tfidf4": tfidf4_unigrams, "unigram_tfidf5": tfidf5_unigrams,
            "bigram_tfidf": tfidf_bigrams, "bigram_tfidf2": tfidf2_bigrams, "bigram_tfidf3": tfidf3_bigrams, "bigram_tfidf4": tfidf4_bigrams, "bigram_tfidf5": tfidf5_bigrams,
            "ubigram_tfidf": tfidf_ubigrams, "ubigram_tfidf2": tfidf2_ubigrams, "ubigram_tfidf3": tfidf3_ubigrams, "ubigram_tfidf4": tfidf4_ubigrams, "ubigram_tfidf5": tfidf5_ubigrams,
            "word2vec": word2vec_embed, "bert_embeddings": bert_embeddings}
ALL_TEST = {"unigram_tfidf": tfidf_unigramsTest, "unigram_tfidf2": tfidf2_unigramsTest, "unigram_tfidf3": tfidf3_unigramsTest, "unigram_tfidf4": tfidf4_unigramsTest, "unigram_tfidf5": tfidf5_unigramsTest,
            "bigram_tfidf": tfidf_bigramsTest, "bigram_tfidf2": tfidf2_bigramsTest, "bigram_tfidf3": tfidf3_bigramsTest, "bigram_tfidf4": tfidf4_bigramsTest, "bigram_tfidf5": tfidf5_bigramsTest,
            "ubigram_tfidf": tfidf_ubigramsTest, "ubigram_tfidf2": tfidf2_ubigramsTest, "ubigram_tfidf3": tfidf3_ubigramsTest, "ubigram_tfidf4": tfidf4_ubigramsTest, "ubigram_tfidf5": tfidf5_ubigramsTest,
            "word2vec": word2vec_embedTest, "bert_embeddings": bert_embeddingsTest}



"""## Logistic Model

Remember: Data is tfidf_unigrams for unigram tfidf, tfidf_bigrams for bigram tfidf, word2vec_embed for word2vec, bert_embeddings for BERT. All of the testing data is the same as train + Test
"""

ALL_DATA = {"unigram_tfidf": tfidf_unigrams, "bigram_tfidf": tfidf_bigrams, "word2vec": word2vec_embed, "bert_embeddings": bert_embeddings}
ALL_TEST = {"unigram_tfidf": tfidf_unigramsTest, "bigram_tfidf": tfidf_bigramsTest, "word2vec": word2vec_embedTest, "bert_embeddings": bert_embeddingsTest}

y = TextAndNBA[f"AoBPR"][TrainIndex].values

X = bert_embeddings



logreg = LogisticRegression(penalty='l1',max_iter=300, solver='liblinear')


# Fit the model to the training data
#logreg.fit(X_train, y_train)

# Predict the labels for the test data
#y_pred = logreg.predict(X_test)

scores = cross_validate(logreg, X, y, cv=10, return_estimator=True)

print(scores['estimator'][4].predict(X))
print(scores['test_score'].mean())
np.argmax(scores['test_score'])

perf_bests = []
perf_models = []
perf_model_names = []

for perf in Performance:
  y = TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), f'AoB{perf}'].values
  best_performance = 0
  best_scores = None
  best_embedding = ""
  for embeddingType in ALL_DATA.keys():
    X = ALL_DATA[embeddingType]
    logreg = LogisticRegression(penalty='l1',max_iter=300, solver='liblinear')
    scores = cross_validate(logreg, X, y, cv=10, return_estimator=True)
    mean_performance = scores['test_score'].mean()
    if mean_performance > best_performance:
      best_performance = mean_performance
      best_scores = scores
      best_embedding = embeddingType

  print(f"Best performance for AoB{perf}: {best_performance}")
  print(f"Used model {best_embedding}")


  bestModelIndex = np.argmax(best_scores['test_score'])
  perf_bests.append(best_performance)
  perf_models.append(best_scores["estimator"][bestModelIndex])
  perf_model_names.append(best_embedding)

for performanceIndex, perf in enumerate(Performance):
  y_test = TextAndNBA.loc[~TextAndNBA.index.isin(TrainIndex), f'AoB{perf}'].values
  embedding = perf_model_names[performanceIndex]
  X_test = ALL_TEST[embedding]
  current_model = perf_models[performanceIndex]
  y_pred = current_model.predict(X_test)
  accuracy = (y_pred == y_test).mean()
  print(f"Using {embedding}, the test accuracy for AoB{perf} was: {accuracy}")

"""## AdaBoost Classifier"""

array = np.arange(1, 101, 4)
print(array)

perf_bests = []
perf_models = []
perf_model_names = []
num_estimators = list(range(0, 111, 5))
num_estimators[0] = 1

best_estimators = []
best_features = []


num_features = ["sqrt", "log2", "special", "auto"]

for perf in Performance:
  y = TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), f'AoB{perf}'].values
  best_performance = 0
  best_scores = None
  best_embedding = ""
  best_esti = 0
  best_feature = None

  for num_feature in num_features:
    for num_estimator in num_estimators:
      for embeddingType in ALL_DATA.keys():
        X = ALL_DATA[embeddingType]

        if num_feature == "special":
          num_feature = int(X.shape[1] / 3)

        sample_tree = DecisionTreeClassifier(max_features=num_feature)
        adareg = AdaBoostClassifier(estimator = sample_tree, n_estimators= num_estimator)
        scores = cross_validate(adareg, X, y, cv=10, return_estimator=True)
        mean_performance = scores['test_score'].mean()
        if mean_performance > best_performance:
          best_performance = mean_performance
          best_scores = scores
          best_embedding = embeddingType
          best_esti = num_estimator
          best_feature = num_feature
  print(f"Best performance for AoB{perf}: {best_performance}")
  print(f"Used model {best_embedding}")
  print(f"Best Estimator {best_esti}")
  print(f"Best Feature {best_feature}")


  best_features.append(best_feature)
  best_estimators.append(best_esti)
  bestModelIndex = np.argmax(best_scores['test_score'])
  perf_bests.append(best_performance)
  perf_models.append(best_scores["estimator"][bestModelIndex])
  perf_model_names.append(best_embedding)

"""### Linear Regression"""

perf_bests = []
perf_models = []
perf_model_names = []

for perf in Performance:
  y = TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), f'{perf}'].values
  best_performance = -100000000
  best_scores = None
  best_embedding = ""
  for embeddingType in ALL_DATA.keys():
    X = ALL_DATA[embeddingType]
    linreg = LinearRegression()
    scores = cross_validate(linreg, X, y, cv=10, return_estimator=True)
    mean_performance = scores['test_score'].mean()
    if mean_performance > best_performance:
      best_performance = mean_performance
      best_scores = scores
      best_embedding = embeddingType

  print(f"Best performance for {perf}: {best_performance}")
  print(f"Used model {best_embedding}")


  bestModelIndex = np.argmax(best_scores['test_score'])
  perf_bests.append(best_performance)
  perf_models.append(best_scores["estimator"][bestModelIndex])
  perf_model_names.append(best_embedding)

"""## Lasso regression"""

perf_bests = []
perf_models = []
perf_model_names = []
perf_alphas = []
for perf in Performance:
  y = TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), f'{perf}'].values
  best_performance = -100000000
  best_scores = None
  best_embedding = ""
  best_alpha = 0
  alpha_values = [ 1e-1, 0.5, 1]
  for alpha in alpha_values:
    for embeddingType in ALL_DATA.keys():
      X = ALL_DATA[embeddingType]
      lassoreg = Lasso(alpha, max_iter=3000)
      scores = cross_validate(lassoreg, X, y, cv=10, return_estimator=True)
      mean_performance = scores['test_score'].mean()
      if mean_performance > best_performance:
        best_performance = mean_performance
        best_scores = scores
        best_embedding = embeddingType
        best_alpha = alpha
  print(f"Best performance for {perf}: {best_performance}")
  print(f"Used model {best_embedding}")
  print(f"Used alpha {best_alpha}")

  bestModelIndex = np.argmax(best_scores['test_score'])
  perf_bests.append(best_performance)
  perf_models.append(best_scores["estimator"][bestModelIndex])
  perf_model_names.append(best_embedding)
  perf_alphas.append(best_alpha)

"""## Ridge Regression"""

perf_bests = []
perf_models = []
perf_model_names = []
perf_alphas = []
for perf in Performance:
  y = TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), f'{perf}'].values
  best_performance = -100000000
  best_scores = None
  best_embedding = ""
  best_alpha = 0
  alpha_values = [1e-3, 5e-2, 1e-2, 1e-1, 0.5, 1, 5, 10, 50, 100]
  for alpha in alpha_values:
    for embeddingType in ALL_DATA.keys():
      X = ALL_DATA[embeddingType]
      ridgereg = Ridge(alpha, max_iter=3000)
      scores = cross_validate(ridgereg, X, y, cv=10, return_estimator=True)
      mean_performance = scores['test_score'].mean()
      if mean_performance > best_performance:
        best_performance = mean_performance
        best_scores = scores
        best_embedding = embeddingType
        best_alpha = alpha
  print(f"Best performance for {perf}: {best_performance}")
  print(f"Used model {best_embedding}")
  print(f"Used alpha {best_alpha}")

  bestModelIndex = np.argmax(best_scores['test_score'])
  perf_bests.append(best_performance)
  perf_models.append(best_scores["estimator"][bestModelIndex])
  perf_model_names.append(best_embedding)
  perf_alphas.append(best_alpha)

"""## Adaboosting Linear Regression"""

perf_bests = []
perf_models = []
perf_model_names = []

#num_estimators = list(range(0, 111, 5))
#num_estimators[0] = 1
num_estimators = [10, 25, 50, 75, 100]


best_estimators = []
best_features = []


num_features = ["sqrt", "log2", "special"]



for perf in Performance:
  y = TextAndNBA.loc[TextAndNBA.index.isin(TrainIndex), f'{perf}'].values
  best_performance = -10000000
  best_scores = None
  best_embedding = ""
  best_esti = 0
  best_feature = None

  for num_feature in num_features:
    #print(f"Doing {num_feature}")
    for num_estimator in num_estimators:
      #print(f"Doing {num_estimator}")
      for embeddingType in ALL_DATA.keys():
        X = ALL_DATA[embeddingType]

        if num_feature == "special":
          num_feature = int(X.shape[1] / 3)

        sample_tree = DecisionTreeRegressor(max_features=num_feature)
        adareg = AdaBoostRegressor(estimator = sample_tree, n_estimators= num_estimator)
        scores = cross_validate(adareg, X, y, cv=10, return_estimator=True)
        mean_performance = scores['test_score'].mean()
        if mean_performance > best_performance:
          best_performance = mean_performance
          best_scores = scores
          best_embedding = embeddingType
          best_esti = num_estimator
          best_feature = num_feature
  print(f"Best performance for {perf}: {best_performance}")
  print(f"Used model {best_embedding}")
  print(f"Best Estimator {best_esti}")
  print(f"Best Feature {best_feature}")


  best_features.append(best_feature)
  best_estimators.append(best_esti)
  bestModelIndex = np.argmax(best_scores['test_score'])
  perf_bests.append(best_performance)
  perf_models.append(best_scores["estimator"][bestModelIndex])
  perf_model_names.append(best_embedding)



